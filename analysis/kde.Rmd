---
title: "KDE"
author: "Shaine"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    number_sections: true
    toc: true
    toc_float:
      toc_collapsed: false
    fig_caption: true
    highlight: tango
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.align = "center", fig.width = 5, fig.height = 3, comment="", warning = FALSE, message = FALSE)
```

# Kernel Density Estimation

This is from @mariarizzo.

Let

```{r}
# Generate data
set.seed(123)
x_data <- rnorm(100, mean = 3, sd = 1)

# Parameters
x_point <- 3
h <- 0.5 # arbitrary
n <- length(x_data)
```

If a histogram with bin width $h$ is constructed from a sample $X_1, \dots, X_n$, then a density estimate for a point $x$ within the range of the data is

$$
\hat{f}(x) = \frac{1}{2hn} \times k
$$

where $k$ is the number of sample points in the interval $\left( x-h, x+h\right)$. This estimator can be written as

$$
\hat{f}(x) = \frac{1}{n} \sum^n_{i=1} \frac{1}{h}w \left( \frac{x - X_i}{h} \right) (\#eq:nde)
$$

where $w(t) =  \frac{1}{2} I(|t| < 1)$ is a weight function. The density estimator $\hat{f}(x)$ in \@ref(eq:nde) with $w(t) =  \frac{1}{2} I(|t| < 1)$ is called naive density estimator. This weight function has the property that $\int^{\infty}_{-\infty} K(t) dt = 1$, and $w(t) \geq 0$, so $w(t)$ is a probability density supported on the interval $[-1, 1]$.

```{r}
# weight function (naive)
naive_kernel <- function(t) {
  if (abs(t) < 1) return(0.5)
  else return(0)
}

# use the weight function to compute weight for each data point
# you check if t, not your raw point, is within the -1 to 1 window
# x_data is Xi in the formula
t <- (x_point - x_data) / h
weights <- sapply(t, naive_kernel)


# naive density estimate for a point x
f_hat <- (1 / (n * h)) * sum(weights)
print(f_hat)


```

```{r}
weights
```


```{r}
# pts inside the window
in_window <- x_data[abs((x_point - x_data) / h) < 1]
hist(x_data, breaks = 20, col = "gray90", border = "gray40",
     freq = FALSE, main = "Naive KDE Estimate at x = 3", xlab = "x")
abline(v = x_point, col = "blue", lty = 2, lwd = 2)              # Point of estimate
points(x_point, f_hat, col = "red", pch = 19, cex = 1.5)
text(x_point, f_hat, labels = round(f_hat, 3), pos = 3, col = "red")
```

```{r fig.height=4}

# Naive KDE function
naive_kde <- function(x, x_data, h) {
  n <- length(x_data)
  weights <- sapply(x_data, function(Xi) {
    t <- (x - Xi) / h
    naive_kernel(t)
  })
  return((1 / (n * h)) * sum(weights))
}

# KDE on a grid: take the KDEstimate for each point in the data
x_grid <- seq(min(x_data) - 1, max(x_data) + 1, length.out = 200)
kde_vals <- sapply(x_grid, naive_kde, x_data = x_data, h = h)


hist(x_data, breaks = 20, col = "gray90", border = "gray40",
     freq = FALSE, main = "Naive KDE vs R::density()",
     xlab = "x", xlim = range(x_grid), ylim = c(0, max(kde_vals)*1.5))

# Overlay manual KDE
lines(x_grid, kde_vals, col = "blue", lwd = 2)

# Overlay R's built-in KDE with rectangular kernel
lines(density(x_data, bw = h, kernel = "rectangular"), col = "red", lwd = 2, lty = 2)
rug(x_data, col = "gray40")
legend("topright", legend = c("Naive KDE (manual)", "R: density() with rectangular kernel"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2, cex = 0.5)

```

::: {style="border-left: 4px solid #007ACC; padding: 1em; background: #f9f9f9;"}
**Aside:**\
This function is 1 when $t$ is between $-1$ and $1$, and $0$ outside that range.

\begin{align*}
I(|t|<1) = 
\begin{cases}
  1 & \text{if} |t| < 1 \\
  0 & \text{otherwise}
\end{cases}
\end{align*}

This weight function is $\frac{1}{2}$ when $t$ is between $-1$ and $1$, and $0$ elsewhere.

$$
w(t) =  \frac{1}{2} I(|t| < 1)
$$

The below illustration of the kernel shows it is a valid PDF with area under the curve 1.

```{r echo = FALSE}
# Create a sequence of t values from -3 to 3
t <- seq(-3, 3, length.out = 500)

# Define the weight function w(t)
w <- ifelse(abs(t) < 1, 0.5, 0)

plot(t, w, type = "l", lwd = 1, col = "blue",
     xlab = "t", ylab = "w(t)",
     main = "Naive Kernel Function\n w(t) = 0.5 if |t| < 1, else 0")
abline(v = -1, col = "red", lty = 2)
abline(v = 1, col = "red", lty = 2)

```
:::

Kernel density estimation replaces the weight function $w(t)$ in the naive estimator with a function $K(\cdot)$ called a kernel function such that.

$$
\int^{\infty}_{-\infty} K(t) dt = 1
$$

In probability density estimation, $K(\cdot)$ is usually a symmetric probability density function. The weight function $w(t)=\frac{1}{2} I(|t| < 1)$ is called the rectangular kernel. The rectangular kernel is a symmetric probability density centered at the origin, and

$$
\frac{1}{nh} w \left(\frac{x-X_i}{h} \right)
$$ corresponds to a rectangle of area $\frac{1}{n}$ centered at $X_i$. The density estimate at $x$ is the sum of rectangles located within $h$ units from $x$.

Restricting our attention to symmetric positive kernel density estimators, suppose that $K(\cdot)$ is another symmetric probability density centered at the origin, and define

$$
\hat{f}_K(x) = \frac{1}{n} \sum^n_{i=1} \frac{1}{h} K \left( \frac{x-X_i}{h} \right) (\#eq:spd)
$$

The, $\hat{f}$ is a probability density function. For example, $K(x)$ may be the triangular density on $[âˆ’1, 1]$ (the triangular kernel) or the standard normal density (the Gaussian kernel). The triangular kernel estimator corresponds to the sum of areas of triangles instead of rectangles. The Gaussian kernel estimator centers a normal density at each data point. The histogram density estimator corresponds to the rectangular kernel density estimator.

The bin width $h$ is a smoothing parameter; small values of $h$ reveal local features of the density, while large values of $h$ produce a smoother density estimate. In kernel density estimation $h$ is called the bandwidth, smoothing parameter or window width.

```{r echo=FALSE, eval = FALSE}
library(shiny)

# Generate sample data
set.seed(123)
x_data <-rexp(1000, 1)# c(-0.77, -0.60, -0.25, 0.14, 0.45, 0.64, 0.65, 1.19, 1.71, 1.74)
sliderInput("h", "Bandwidth (h):", min = 0.1, max = 2, value = 0.4, step = 0.05)
selectInput("kernel", "Kernel:", choices = c("gaussian","epanechnikov", "rectangular", "triangular", "biweight", "optcosine"), selected = "gaussian")


renderPlot({
  h <- input$h
  kernel_type <- input$kernel
  n <- length(x_data)
  x_grid <- seq(min(x_data) - 1, max(x_data) + 1, length.out = 300)
  
  kernel_variance <- switch(kernel_type,
  "gaussian" = 1,
  "epanechnikov" = 1/5,
  "rectangular"  = 1/3,
  "triangular" = 1/6,
  "biweight" = 1/7,
  "optcosine" = 1-8/pi^2
)

  variance_scaling <- sqrt(kernel_variance)
  adjusted_h <- h / variance_scaling 


  ## Normalize a kernel function to integrate to 1 over its support
  # normalize_kernel <- function(f, lower = -1, upper = 1) {
  #   norm_const <- integrate(f, lower, upper)$value
  #   function(t) f(t) / norm_const
  # }

  # Raw kernel functions
  raw_kernel_function <- switch(kernel_type,
    "gaussian" = function(t) (1 / sqrt(2 * pi)) * exp(-0.5 * t^2),
    "epanechnikov" = function(t) ifelse(abs(t) <= 1, (3/4)*(1 - t^2), 0),
    "rectangular"  = function(t) ifelse(abs(t) <= 1, 1/2, 0),
    "triangular" = function(t) ifelse(abs(t) <= 1, 1 - abs(t), 0),
    "biweight" = function(t) ifelse(abs(t) <= 1, (15/16)*(1 - t^2)^2, 0),
    "optcosine" = function(t) ifelse(abs(t) <= 1, (pi/4)*cos((pi*t)/2),0)
  )

  # Integration limits (Gaussian is over whole line)
  # integration_limits <- if (kernel_type == "gaussian") c(-Inf, Inf) else c(-1, 1)

  # Normalize the kernel function
  kernel_function <- raw_kernel_function
  # normalize_kernel(
  #   raw_kernel_function,
  #   lower = integration_limits[1],
  #   upper = integration_limits[2]
  # )

  # Manual KDE
  kde_vals <- sapply(x_grid, function(x) {
    sum(sapply(x_data, function(Xi) {
      
      t <- (x - Xi) / adjusted_h
      
      kernel_function(t) / adjusted_h
    })) / n
  })

  # Plot setup
  plot(x_grid, kde_vals, type = "n", xlab = "x", ylab = "Density",
       main = paste("Kernel:", kernel_type, " | Bandwidth:", round(h, 2)),
       ylim = c(0, max(kde_vals)*1.2))

  hist(x_data, probability = TRUE, col = rgb(0.8, 0.8, 0.8, 0.4),
     border = "white", add = TRUE, breaks = "FD")  # or use breaks = 10
  
  # add the true density to compare
  y <- seq(.001, 6, .01)
  lines(y, dexp(y, 1), lty = 2)
  
  # Individual kernel bumps
  for (Xi in x_data) {
    bump_x <- seq(Xi - 3*h, Xi + 3*h, length.out = 100)
    bump_y <- sapply(bump_x, function(x) {
      t <- (x - Xi) / adjusted_h
      kernel_function(t) / (adjusted_h * n)
    })
    lines(bump_x, bump_y, col = rgb(0.2, 0.2, 0.8, 0.3))
  }

  # KDE curve
  lines(x_grid, kde_vals, col = "blue", lwd = 2)

  # R's density function for comparison
  r_dens <- density(
    x_data, bw = h, kernel = kernel_type,
    n = length(x_grid), from = min(x_grid), to = max(x_grid)
  )
  lines(r_dens$x, r_dens$y, col = "red", lwd = 2, lty = 2)

  rug(x_data, col = "gray40")
})

```


```{r echo = FALSE, eval = FALSE}
library(MASS)
radioButtons(
  "hs",
  "Bandwidth (h):",
  choices = c('3.998', '2','5','7'),
  selected = '3.998', inline = TRUE
)


waiting <- geyser$waiting
n <- length(waiting)
h1 <- 1.06 * sd(waiting) * n^(-1/5)
h2 <- .9 * min(c(IQR(waiting)/1.34, sd(waiting))) * n^(-1/5)

renderPlot({
  h_val <- as.numeric(input$hs)
  plot(density(waiting, bw=h_val), main = paste("Geyser Waiting Data: Bandwidth(",h_val, ")"))
})

```



# Uncertainty for ranks

Let $\theta_1, \theta_2, \dots, \theta_K$ be the true values of the parameters and let $\theta_{(1)}, \theta_{(2)}, \dots, \theta_{(K)}$ be the corresponding ordered parameter values.


Let $F_i$ be the CDF of $\hat{\theta}_{(i)} - \theta_{(i)}, i = 1, 2, \dots, K$. Note that $F_i \left( \hat{\theta}_{(i)} - \theta_{(i)}\right) \sim U(0,1)$. We want the quantity $u$ that satisfies :

$$
P \left( 1-u < F_i \left ( \hat{\theta}_{(i)}-\theta_{(i)} \right) < u, \forall i\right) = 1 - \alpha
\\
\Longleftrightarrow
P \left( 
\underset{1 \leq i \leq p} {\max}  
\left\{ \max
\left\{
F_i
\left( \hat{\theta}_{(i)} - \theta_{(i)}
\right),
1-F_i
\left( \hat{\theta}_{(i)} - \theta_{(i)}
\right)
\right\}
\right\} < u
\right) = 1 - \alpha
$$


Taking the inverse,

$$
F_i^{-1} (1-u) <  \hat{\theta}_{(i)}-\theta_{(i)}  < F_i^{-1} (u), i = 1, 2, \dots, K
$$

Isolating the true value,

$$
F_i^{-1} (1-u) - \hat{\theta}_{(i)} <  -\theta_{(i)}  < F_i^{-1} (u)-\hat{\theta}_{(i)}
\\
\hat{\theta}_{(i)} - F_i^{-1} (u) < \theta_{(i)} < \hat{\theta}_{(i)} - F_i^{-1} (1-u) 
$$

Problem 1: We do not know $F_i^{-1}$
Problem 2: We do not know $u$


```{r}
alpha <- 0.10
K <- 10
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")
calculate_variance <- function(moe) (moe/1.645)^2
dataset <- df[order(df$k),][1:K,]
dataset['variance'] <- lapply(dataset['moe_k'], calculate_variance)
rownames(dataset) <- dataset[,2]
```


## Algorithm 1

### Steps

Suppose the data consist of $\hat{\theta}_1, \hat{\theta}_1, \dots, \hat{\theta}_K$ and known values $\sigma_1^2, \sigma_2^2, \dots, \sigma_K^2$

1. Generate $\hat{\theta}^*_{bi} \sim \mathcal{N}(\hat{\theta}_i, \sigma_i^2)$


```{r results='asis'}
library(knitr)
library(kableExtra)

B=1000
library(foreach)
set.seed(4)
mat1 <- foreach(i = 1:K, .combine = cbind) %do% {
  foreach(b = 1:B, .combine = c) %do% {
    rnorm(1, mean = dataset[i, 'theta_k'], sd = sqrt(dataset[i, 'variance']))
  }
}

kable(head(mat1)) %>% kable_styling(bootstrap_options = "responsive")
```

2. For each $b$, order the set $\left\{ \hat{\theta}^*_{b1}, \hat{\theta}^*_{b2}, \dots, \hat{\theta}^*_{bK} \right\}$ to obtain $\left\{ \hat{\theta}^*_{b(1)}, \hat{\theta}^*_{b(2)}, \dots, \hat{\theta}^*_{b(K)} \right\}$


```{r results='asis'}
source('../R/algo1_helper.R')
mat1_sorted <- t(apply(mat1, 1, sort))

kable(head(mat1_sorted)) %>% kable_styling(bootstrap_options = "responsive")
```

3. For each $b$, compute the difference $r^*_{bi} = \hat{\theta}^*_{b(i)} - \hat{\theta}^*_{(i)}, i = 1, 2, \dots, K$

```{r results='asis'}
r <- mat1_sorted - matrix(sort(t(dataset['theta_k'])),B, K, byrow=TRUE)
kable(head(r)) %>% kable_styling(bootstrap_options = "responsive")
```

4. Let $F_i$ be the density of $\hat{\theta}_{(i)} - \theta_{(i)}, i = 1, 2, \dots, K$. Estimate each $F_i$ bu computing $\hat{F}_i$ using KDE based on $r^*_{1i}, r^*_{2i}, \dots, r^*_{Bi}$, that is

$$
\hat{F}_i(x) = \frac{1}{B}\sum^B_{\alpha=1} \Phi \left( \frac{x-r^*_{\alpha i}}{h_i} \right), \ \ \ h_i=0.9\min\{S_i, IQR_i/1.34\}\ B^{-1/5}
$$
```{r}
#source('../R/algo1_helper.R')
# kde_cdf_function <- function(datapoints, bandwidth) {
#   return(function(x) mean(pnorm((x - datapoints) / bandwidth)))
# }
# 
# estimate_CDF <- function(dataset, i){
#   S = sqrt(dataset[i,'variance'])#sd(r[,i])
#   IQR = quantile(r[,i], .75) - quantile(r[,i], .25)
#   h_i <- 0.9*min(S, IQR/1.34)*(B^(1/5))
#   return(kde_cdf_function(r[,i], h_i))
# }

estimated_CDF <- lapply(1:K, function(x)(estimate_CDF(dataset, x, r)))
```

5. Compute $Y^*_{bi}= \hat{F}_i(r^*_{bi})$ for each $b = 1, 2, \dots, B$ and $i = 1, 2, \dots, K$

```{r results='asis'}
# get_inner_max <- function(value) return(max(value, 1-value))

Y <- foreach(i = 1:K, .combine = cbind) %do% {
  sapply(r[,i], estimated_CDF[[i]])
}

kable(head(Y)) %>% kable_styling(bootstrap_options = "responsive")
```

6. Compute $U^*_b = \underset{1 \leq i \leq p} {\max} \{\max\{Y^*_{bi}, 1-Y^*_{bi}\}\}$ for $b = 1, 2, \dots, B$

```{r results='asis'}
U <- apply(apply(Y, MARGIN = c(1, 2), FUN = get_inner_max), 1, max)
head(U)
```

7. Compute $\hat{u}$ as the $(1-\alpha)$-sample quantile of $U^*_1, U^*_2, \dots, U^*_B$

```{r}
uhat <- quantile(U, probs = 1 - alpha)
uhat
```

8. The join confidence region is calculated

```{r results='asis'}
library("GoFKernel")

F.inv <- lapply(estimated_CDF, function(F){inverse(F, lower = -100, upper = 100)})
dataset['F.inv_u'] <- sapply(1:K, function(i){F.inv[[i]](uhat)})
dataset['F.inv_1-u'] <- sapply(1:K, function(i){F.inv[[i]](1-uhat)})

dataset['kde_ci_lower'] <- dataset['theta_k'] - dataset['F.inv_u'] 
dataset['kde_ci_upper'] <- dataset['theta_k'] - dataset['F.inv_1-u'] 

kable(head(dataset)) %>% kable_styling(bootstrap_options = "responsive")
```

### Full function


```{r}
source('../R/algo1_helper.R')
library(foreach)
library("GoFKernel")

alpha <- 0.10
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")
```


```{r}
dataset <- df[order(df$k),][1:30,]
B=1000

run_algorithm1 <- function(B, dataset, seed = 4) {
  dataset['variance'] <- lapply(dataset['moe_k'], calculate_variance)
  K <- dim(dataset)[1]
  set.seed(seed)
  mat1 <- foreach(i = 1:K, .combine = cbind) %do% {
    foreach(b = 1:B, .combine = c) %do% {
      rnorm(1, mean = dataset[i, 'theta_k'], sd = sqrt(dataset[i, 'variance']))
    }
  }
  # 2
  mat1_sorted <- t(apply(mat1, 1, sort))
  # 3
  r <- mat1_sorted - matrix(sort(t(dataset['theta_k'])),B, K, byrow=TRUE)
  # 4
  estimated_CDF <- lapply(1:K, function(x)(estimate_CDF(dataset, x, r)))
  # 5
  Y <- foreach(i = 1:K, .combine = cbind) %do% {
    sapply(r[,i], estimated_CDF[[i]])
  }
  # 6
  U <- apply(apply(Y, MARGIN = c(1, 2), FUN = get_inner_max), 1, max)
  # 7
  uhat <- quantile(U, probs = 1 - alpha)
  # 8
  
  F.inv <- lapply(estimated_CDF, function(F){inverse(F, lower = -100, upper = 100)})
  dataset['F.inv_u'] <- sapply(1:K, function(i){F.inv[[i]](uhat)})
  dataset['F.inv_1-u'] <- sapply(1:K, function(i){F.inv[[i]](1-uhat)})
  dataset['kde_ci_lower'] <- dataset['theta_k'] - dataset['F.inv_u'] 
  dataset['kde_ci_upper'] <- dataset['theta_k'] - dataset['F.inv_1-u']
  return(dataset[, c('rhat_k', 'k', 'theta_k', 'kde_ci_lower', 'kde_ci_upper')])
}
```

### Application to diff K
```{r eval = FALSE}

alpha <- 0.10
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")

B=1000

for (K in c(10, 20, 30, 40, 51)) {
  dataset <- df[order(df$k),][1:K,]
  result <- run_algorithm1(B, dataset)

  filename <- paste0("../data/kde_intervals_", K, ".rds")
  saveRDS(result, file = filename)
}
```


```{r results='asis'}
get_ranks <- function(k, tuple_list){
  Lambda_lk <- which(tuple_list[,2]<=tuple_list[k,1])
  Lambda_lk <- Lambda_lk[Lambda_lk != k]
  Lambda_Ok <- which(tuple_list[,2]>tuple_list[k,1] & tuple_list[k,2] > tuple_list[,1])
  Lambda_Ok <- Lambda_Ok[Lambda_Ok != k]

  # return(paste(seq(length(unique(Lambda_lk))+1, 
  #     length(unique(Lambda_lk)) + length(unique(Lambda_Ok)) + 1, 
  #     1), collapse = ", "))
  ranks <- seq(
    length(unique(Lambda_lk)) + 1,
    length(unique(Lambda_lk)) + length(unique(Lambda_Ok)) + 1,
    1
  )
  
  return(list(
    ranks = ranks,
    Lambda_Ok = Lambda_Ok
  ))
  
  
}

output <- list()

for (K in c(10, 20, 30, 40, 51)) {
  filename <- paste0("../data/kde_intervals_", K, ".rds")
  output[[K]] <- readRDS(filename)
  tuple_list <- t(apply(output[[K]][,c('kde_ci_lower', 'kde_ci_upper')], 1, function(row) as.numeric(row)))
  
  output[[K]]['kde_rank_lower'] <- sapply(1:K, function(x) min(get_ranks(x, tuple_list)$ranks))
  output[[K]]['kde_rank_upper'] <- sapply(1:K, function(x) max(get_ranks(x, tuple_list)$ranks))
  output[[K]]['kde_Lambda_Ok_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$Lambda_Ok))
  output[[K]]['kde_rank_range_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$ranks))
  print(paste("**** Output shown for K=", K, "****"))
  
  output[[K]]['sample_rank'] <- rank(output[[K]]$theta_k, ties.method = "max")
  print(kable(output[[K]][order(output[[K]]$theta_k), ], row.names = FALSE) %>% kable_styling(bootstrap_options = "responsive"))
  cat("\n\n---\n\n")
}
```


```{r}

```


```{r fig.height = 8, fig.width=10, echo = FALSE}
kde_result <- output[[51]]
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")
df$order_index <- seq(K, 1, -1)

df$iso <- c('MD', 'NY', 'NJ', 'DC','IL', 'MA', 'VA', 'CA', 'GA', 'NH', 'PA', 'FL', 'HI', 'WV','WA', 'DE', 'CT', 'AZ', 'TX', 'CO','LA', 'TN', 'MI', 'NV', 'AL', 'MS', 'SC', 'IN', 'ME', 'NC', 'RI', 'MO', 'OH', 'MN', 'KY', 'OR', 'VT', 'WI', 'UT', 'NM', 'AR', 'OK', 'ID', 'KS', 'IA', 'AK', 'MT', 'NE', 'WY', 'ND', 'SD'  )

kde_result <- merge(x = df[,c('k', 'iso', 'order_index')], y = kde_result, by = "k", all.x = TRUE)

library(ggplot2)
library(dplyr)
library(tidyr)

to_string_seq <- function(x, y){paste(seq(x, y, 1), collapse=',')}

dat_to_plot <- kde_result %>% 
    select(c(k, iso, kde_rank_lower, kde_rank_upper, sample_rank, order_index)) %>%
    mutate(string_ranks = mapply(to_string_seq, kde_rank_lower, kde_rank_upper)) %>%
    separate_rows(string_ranks, sep=",") %>%
    select(c(k, iso, sample_rank, string_ranks, order_index))

dat_to_plot$highlight <- ifelse(dat_to_plot$sample_rank == as.numeric(dat_to_plot$string_ranks), "highlight", "normal")

p <- ggplot(data = dat_to_plot, aes(x = reorder(k, order_index),
                               y = as.numeric(string_ranks),
                               label = iso,
                               color = highlight)) + geom_text(size = 2) +
  scale_color_manual(values = c("highlight" = "red", "normal" = "black")) +
  scale_y_continuous(breaks = 1:51) +
  labs(title='(KDE) visualization of the 90% joint confidence') + xlab('States') + ylab('Sample rank') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        ) +
  guides(colour=FALSE) +
    theme(axis.line = element_line(colour = "gray"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),)

#ggsave("../figures/kde_ranks_plot.png", plot = p, width = 10, height = 8, dpi = 300)
p
```



```{r fig.height = 8, fig.width=10, echo = FALSE}
to_string_seq <- function(x, y){paste(seq(x, y, 1), collapse=',')}

dat_to_plot <- df %>% 
    select(c(k, iso, ind_rank_lower, ind_rank_upper, rhat_k, order_index)) %>%
    mutate(string_ranks = mapply(to_string_seq, ind_rank_lower, ind_rank_upper)) %>%
    separate_rows(string_ranks, sep=",") %>%
    select(c(k, iso, rhat_k, string_ranks, order_index))

dat_to_plot$highlight <- ifelse(dat_to_plot$rhat_k == as.numeric(dat_to_plot$string_ranks), "highlight", "normal")

p <- ggplot(data = dat_to_plot, aes(x = reorder(k, order_index),
                               y = as.numeric(string_ranks),
                               label = iso,
                               color = highlight)) + geom_text(size = 2) +
  scale_color_manual(values = c("highlight" = "red", "normal" = "black")) +
  scale_y_continuous(breaks = 1:51) +
  labs(title='(IND) visualization of the 90% joint confidence') + xlab('States') + ylab('Sample rank') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        ) +
  guides(colour=FALSE) +
    theme(axis.line = element_line(colour = "gray"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),)

#ggsave("../figures/ind_ranks_plot.png", plot = p, width = 10, height = 8, dpi = 300)
p
```

## T1, T2, T3


```{r}
tuple_list <- t(apply(df[,c('bonf_ci_lower', 'bonf_ci_upper')], 1, function(row) as.numeric(row)))
df['bonf_Lambda_Ok_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$Lambda_Ok))
df['bonf_rank_range_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$ranks))

tuple_list <- t(apply(df[,c('ind_ci_lower', 'ind_ci_upper')], 1, function(row) as.numeric(row)))
df['ind_Lambda_Ok_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$Lambda_Ok))
df['ind_rank_range_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$ranks))
```


```{r results='asis'}
get_t1 <- function(v){mean(v)}
get_t2 <- function(v){prod(v)^(1/length(v))}
get_t3 <- function(v){1 - ((length(v)+sum(v))/(length(v)^2))}

#Oks <- list(df$bonf_Lambda_Ok_length, df$ind_Lambda_Ok_length, output[[51]]$kde_Lambda_Ok_length)
Oks <- list(df$bonf_rank_range_length, df$ind_rank_range_length, output[[51]]$kde_rank_range_length)

kable(data.frame(T1 = sapply(Oks, get_t1),
           T2 = sapply(Oks, get_t2),
           T3 = sapply(Oks, get_t3), row.names = c("bonf", "ind", "kde"))) %>% kable_styling(bootstrap_options = "responsive")
```

## Algorithm 2

```{r}
library(doParallel)
library(tictoc)
alpha <- 0.10
K <- 51
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")
calculate_variance <- function(moe) (moe/1.645)^2
dataset_or <- df[order(df$k),][1:K,]
dataset_or['variance'] <- lapply(dataset_or['moe_k'], calculate_variance)
rownames(dataset_or) <- dataset_or[,2]


cl=parallel::makeCluster(15)
registerDoParallel(cl)

home1 <- function(){
outt <- foreach(iter = 1:5000, .combine = rbind,.packages = c("foreach"), .export = c('alpha', 'K', "df", "dataset_or", "run_algorithm1", 'calculate_variance', "B", "estimate_CDF", "get_ranks", "get_t2", "kde_cdf_function",'get_inner_max', "inverse")) %dopar% {

  dataset <- dataset_or
  set.seed(iter)
  mat2 <- foreach(i = 1:K, .combine = cbind) %do% {
      rnorm(1, mean = dataset[i, 'theta_k'], sd = sqrt(dataset[i, 'variance']))
  }
  
  colnames(dataset)[colnames(dataset) == 'theta_k'] <- 'otheta_k'
  
  dataset$theta_k <- t(mat2)
  result <- run_algorithm1(B, dataset, iter)
  
  tuple_list <- t(apply(result[,c('kde_ci_lower', 'kde_ci_upper')], 1, function(row) as.numeric(row)))
  
  result['ind_rank_range_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$ranks))

  sorted_ci <- result[order(result$theta_k),]
  sorted_theta <- sort(dataset$otheta_k)
  inside_check <- all(sorted_theta >= sorted_ci$kde_ci_lower & sorted_theta <= sorted_ci$kde_ci_upper)
  t2_val <- get_t2(result$ind_rank_range_length)
  
  data.frame(t2 = t2_val, inside = inside_check)
}
}
tic("Running...")
zzz <- home1()
toc()
stopCluster(cl)
saveRDS(zzz,  paste0("../output/coverage_probability_",K, ".rds"))
# 8770.893
```


# Appendices

... checking `get_ranks` fcn is aligned to handout example

```{r}
df<-data.frame(lower = c(2, 4, 8, 9), upper = c(6, 7, 11, 10))
tuple_list <-  t(apply(df[,c('lower', 'upper')], 1, function(row) as.numeric(row)))
```


```{r}
get_ranks(1, tuple_list)
get_ranks(2, tuple_list)
get_ranks(3, tuple_list)
get_ranks(4, tuple_list)

```

```{r eval = FALSE}
# function check
tuple_list <- t(apply(df[,c('bonf_ci_lower', 'bonf_ci_upper')], 1, function(row) as.numeric(row)))
df$bonf_rank_lower == sapply(1:K, function(x) min(get_ranks(x, tuple_list)$ranks))
df$bonf_rank_upper == sapply(1:K, function(x) max(get_ranks(x, tuple_list)$ranks))


tuple_list <- t(apply(df[,c('ind_ci_lower', 'ind_ci_upper')], 1, function(row) as.numeric(row)))
df$ind_rank_lower == sapply(1:K, function(x) min(get_ranks(x, tuple_list)$ranks))
df$ind_rank_upper == sapply(1:K, function(x) max(get_ranks(x, tuple_list)$ranks))
```


# References
