---
title: "KDE"
author: "Shaine"
date: "`r Sys.Date()`"
output:
  bookdown::html_document2:
    number_sections: true
    toc: true
    toc_float:
      toc_collapsed: false
      toc_depth: 4
    fig_caption: true
    highlight: tango
bibliography: references.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,fig.align = "center", fig.width = 5, fig.height = 3, comment="", warning = FALSE, message = FALSE)
```



# Uncertainty for ranks

Let $\theta_1, \theta_2, \dots, \theta_K$ be the true values of the parameters and let $\theta_{(1)}, \theta_{(2)}, \dots, \theta_{(K)}$ be the corresponding ordered parameter values.


Let $F_i$ be the CDF of $\hat{\theta}_{(i)} - \theta_{(i)}, i = 1, 2, \dots, K$. Note that $F_i \left( \hat{\theta}_{(i)} - \theta_{(i)}\right) \sim U(0,1)$. We want the quantity $u$ that satisfies :

$$
P \left( 1-u < F_i \left ( \hat{\theta}_{(i)}-\theta_{(i)} \right) < u, \forall i\right) = 1 - \alpha
\\
\Longleftrightarrow
P \left( 
\underset{1 \leq i \leq p} {\max}  
\left\{ \max
\left\{
F_i
\left( \hat{\theta}_{(i)} - \theta_{(i)}
\right),
1-F_i
\left( \hat{\theta}_{(i)} - \theta_{(i)}
\right)
\right\}
\right\} < u
\right) = 1 - \alpha
$$


Taking the inverse,

$$
F_i^{-1} (1-u) <  \hat{\theta}_{(i)}-\theta_{(i)}  < F_i^{-1} (u), i = 1, 2, \dots, K
$$

Isolating the true value,

$$
F_i^{-1} (1-u) - \hat{\theta}_{(i)} <  -\theta_{(i)}  < F_i^{-1} (u)-\hat{\theta}_{(i)}
\\
\hat{\theta}_{(i)} - F_i^{-1} (u) < \theta_{(i)} < \hat{\theta}_{(i)} - F_i^{-1} (1-u) 
$$

Problem 1: We do not know $F_i^{-1}$
Problem 2: We do not know $u$


```{r}
alpha <- 0.10
K <- 10
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")
calculate_variance <- function(moe) (moe/1.645)^2
dataset <- df[order(df$k),][1:K,]
dataset['variance'] <- lapply(dataset['moe_k'], calculate_variance)
rownames(dataset) <- dataset[,2]
```


```{r echo = FALSE}
# library("RankingProject")
# TravelTime2011
# 
# pck <- merge(df, TravelTime2011[c('State', 'Estimate.2dec', 'SE.2dec')], by.x = 'k', by.y="State")
# names(pck)[names(pck) == "theta_k"] <- "theta_k_COPIED"
# names(pck)[names(pck) == "Estimate.2dec"] <- "theta_k"
# names(pck)[names(pck) == "SE.2dec"] <- "S"
# 
# saveRDS(pck,"../data/mean_travel_time_ranking_2011.rds")
```



## Algorithm 1

### Steps

Suppose the data consist of $\hat{\theta}_1, \hat{\theta}_1, \dots, \hat{\theta}_K$ and known values $\sigma_1^2, \sigma_2^2, \dots, \sigma_K^2$

1. Generate $\hat{\theta}^*_{bi} \sim \mathcal{N}(\hat{\theta}_i, \sigma_i^2)$


```{r results='asis'}
library(knitr)
library(kableExtra)

B <- 1000
library(foreach)
set.seed(4)
thetahat_star <- foreach(i = 1:K, .combine = cbind) %do% {
  foreach(b = 1:B, .combine = c) %do% {
    rnorm(1, mean = dataset[i, 'theta_k'], sd = dataset[i, 'S'])
  }
}

colnames(thetahat_star) <- paste0("thetahat_star", sprintf("%02d", 1:K))

kable(head(thetahat_star)) %>% kable_styling(bootstrap_options = "responsive")
```

```{r fig.height=3, fig.align='center', fig.width=12, echo = FALSE}
library(tidyr)
library(ggplot2)

dat10 <- data.frame(thetahat_star)
x <- paste0("thetahat_star", sprintf("%02d", 1:10))
colnames(dat10) <- x

gather(dat10) %>% ggplot(aes(x = key, y = value)) + geom_boxplot() +
  labs(title='Boxplot of Bootstrap Samples') + xlab('States') + ylab('Boxplot') +
  theme_bw() #+
  #theme(axis.text.x = element_text(angle = 45, hjust = 1),
  #      ) 
```

```{r fig.height=4, fig.align='center', fig.width=12, echo = FALSE}
gather(dat10) %>% ggplot(aes(x = value)) +geom_histogram(alpha = 0.7)+coord_flip() + theme_bw() +
  labs(title='Histogram of BS samples') + xlab('Histogram') + ylab('') +facet_grid(.~key,scales = "free") + guides(fill = FALSE)
```



2. For each $b$, order the set $\left\{ \hat{\theta}^*_{b1}, \hat{\theta}^*_{b2}, \dots, \hat{\theta}^*_{bK} \right\}$ to obtain $\left\{ \hat{\theta}^*_{b(1)}, \hat{\theta}^*_{b(2)}, \dots, \hat{\theta}^*_{b(K)} \right\}$


```{r results='asis'}
source('../R/algo1_helper.R')
sorted_thetahat_star <- t(apply(thetahat_star, 1, sort))
colnames(sorted_thetahat_star) <- paste0("sorted_thetahat_star", sprintf("%02d", 1:10))

kable(head(sorted_thetahat_star)) %>% kable_styling(bootstrap_options = "responsive")
```


```{r fig.height=4, fig.align='center', fig.width=12, echo = FALSE}
dat10 <- data.frame(sorted_thetahat_star)
x <- paste0("thetahat_star(", sprintf("%02d", 1:10), ")")
colnames(dat10) <- x

gather(dat10) %>% ggplot(aes(x = key, y = value)) + geom_boxplot() +
  labs(title='Boxplot of Ordered Bootstrap Samples') + xlab('States') + ylab('Boxplot') +
  theme_bw()
```

```{r fig.height=4, fig.align='center', fig.width=12, echo = FALSE}
gather(dat10) %>% ggplot(aes(x = value)) +geom_histogram(alpha = 0.7)+coord_flip() + theme_bw() +
  labs(title='Histogram of Ordered BS samples') + xlab('Histogram') + ylab('') +facet_grid(.~key,scales = "free_x") + guides(fill = FALSE)
```


3. For each $b$, compute the difference $r^*_{bi} = \hat{\theta}^*_{b(i)} - \hat{\theta}_{(i)}, i = 1, 2, \dots, K$

```{r results='asis'}
rstar <- sorted_thetahat_star - matrix(sort(t(dataset['theta_k'])),B, K, byrow=TRUE)
colnames(rstar) <- paste0("rstar", sprintf("%02d", 1:10))
kable(head(rstar)) %>% kable_styling(bootstrap_options = "responsive")
```

```{r fig.height=4, fig.align='center', fig.width=12, echo = FALSE}
dat10 <- data.frame(rstar)
x <- paste0("rstar", sprintf("%02d", 1:10))
colnames(dat10) <- x

gather(dat10) %>% ggplot(aes(x = key, y = value)) + geom_boxplot() +
  labs(title='Boxplot of rstar') + xlab('States') + ylab('Boxplot') +
  theme_bw()
```

```{r fig.height=4, fig.align='center', fig.width=12, echo = FALSE}
gather(dat10) %>% ggplot(aes(x = value)) + geom_histogram(alpha = 0.7)+coord_flip() + theme_bw() +
  labs(title='Histogram of rstar') + xlab('Histogram') + ylab('') +facet_grid(.~key,scales = "free") + guides(fill = FALSE)
```


4. Let $F_i$ be the density of $\hat{\theta}_{(i)} - \theta_{(i)}, i = 1, 2, \dots, K$. Estimate each $F_i$ bu computing $\hat{F}_i$ using KDE based on $r^*_{1i}, r^*_{2i}, \dots, r^*_{Bi}$, that is

$$
\hat{F}_i(x) = \frac{1}{B}\sum^B_{\alpha=1} \Phi \left( \frac{x-r^*_{\alpha i}}{h_i} \right), \ \ \ h_i=0.9\min\{S_i, IQR_i/1.34\}\ B^{-1/5}
$$

```{r echo = FALSE}
source('../R/algo1_helper.R')
# kde_cdf_function <- function(datapoints, bandwidth) {
#   return(function(x) mean(pnorm((x - datapoints) / bandwidth)))
# }
# 
# estimate_CDF <- function(dataset, i){
#   S = sqrt(dataset[i,'variance'])#sd(r[,i])
#   IQR = quantile(r[,i], .75) - quantile(r[,i], .25)
#   h_i <- 0.9*min(S, IQR/1.34)*(B^(1/5))
#   return(kde_cdf_function(r[,i], h_i))
# }

```

```{r}
Fhat <- lapply(1:K, function(x)(estimate_CDF(x, rstar, B)))
fhat <- lapply(1:K, function(x)(estimate_PDF(x, rstar, B)))
```


```{r fig.height=4, fig.align='center', fig.width=12, echo = FALSE}
xgrid <- seq(-1, 1, length.out = 200)

pdf_data <- do.call(rbind, lapply(1:length(fhat), function(k) {
  data.frame(
    x = xgrid,
    density = fhat[[k]](xgrid),
    key = names(dat10)[k]
  )
}))


gather(dat10) %>%
  ggplot(aes(x = value)) +
  geom_histogram(aes(y = ..density..), bins = 30, alpha = 0.7) +
  geom_line(data = pdf_data, aes(x = x, y = density), color = "red", size = 1) +
  coord_flip() +
  theme_bw() +
  labs(title = 'Histogram of rstar with Estimated PDF Overlay',
       x = 'Value',
       y = 'Density') +
  facet_grid(. ~ key, scales = "free") +
  guides(fill = FALSE)
```





5. Compute $Y^*_{bi}= \hat{F}_i(r^*_{bi})$ for each $b = 1, 2, \dots, B$ and $i = 1, 2, \dots, K$ (PIT part)


```{r results='asis'}
Ystar <- foreach(i = 1:K, .combine = cbind) %do% {
  sapply(rstar[,i], Fhat[[i]])
}
colnames(Ystar) <- paste0("Ystar", sprintf("%02d", 1:10))
kable(head(Ystar)) %>% kable_styling(bootstrap_options = "responsive")
```

```{r fig.height=4, fig.align='center', fig.width=12, echo = FALSE}
dat10 <- data.frame(Ystar)
x <- paste0("Ystar", sprintf("%02d", 1:10))
colnames(dat10) <- x

gather(dat10) %>% ggplot(aes(x = key, y = value)) + geom_boxplot() +
  labs(title='Boxplot of Ystar') + xlab('States') + ylab('Boxplot') +
  theme_bw()
```

```{r fig.height=4, fig.align='center', fig.width=12, echo = FALSE}
# gather(dat10) %>% ggplot(aes(x = value)) +geom_histogram(alpha = 0.7)+coord_flip() + theme_bw() +
#   labs(title='Histogram of Ystar') + xlab('Histogram') + ylab('') +facet_grid(.~key,scales = "free") + guides(fill = FALSE)

gather(dat10) %>%
  ggplot(aes(x = value)) +
  geom_histogram(aes(y = ..density..), bins = 30, alpha = 0.7) +
  stat_function(fun = function(x) dunif(x, min = 0, max = 1), color = "red", size = 1) +
  coord_flip() +
  theme_bw() +
  labs(title = 'Histogram of Ystar with Uniform(0,1) Overlay',
       x = 'Value',
       y = 'Density') +
  facet_grid(. ~ key, scales = "free") +
  guides(fill = FALSE)
```



```{r fig.height=8, fig.align='center', fig.width=20, echo = FALSE}
gather(dat10) %>%
  ggplot(aes(sample = value)) +
  stat_qq(distribution = qunif, dparams = list(min = 0, max = 1)) +
  stat_qq_line(distribution = qunif, dparams = list(min = 0, max = 1), color = "red") +
  facet_wrap(. ~ key, scales = "free", nrow = 2) +
  theme_bw() +
  labs(title = 'Q-Q Plot vs Uniform(0,1)', x = 'Theoretical Quantiles', y = 'Sample Quantiles')
```



```{r}
#apply(dat10, 2, function(col) ks.test(col, "punif", min = 0, max = 1))
```


6. Compute $U^*_b = \underset{1 \leq i \leq p} {\max} \{\max\{Y^*_{bi}, 1-Y^*_{bi}\}\}$ for $b = 1, 2, \dots, B$.

Due to PIT result in item 5, we can be sure that $\max\{Y^*_{bi}, 1-Y^*_{bi}\} \geq 0.5$ since $0 \leq Y^*_{bi}= \hat{F}_i(r^*_{bi}) \leq 1$. It follows that $U^*_b \geq 0.5$

```{r results='asis'}
Ustar <- apply(apply(Ystar, MARGIN = c(1, 2), FUN = get_inner_max), 1, max)
head(Ustar)
```


```{r}
summary(Ustar)
```



```{r}
hist(Ustar)
```


7. Compute $\hat{u}$ as the $(1-\alpha)$-sample quantile of $U^*_1, U^*_2, \dots, U^*_B$

```{r}
# what is the value of uhat ST when we repeat the experiment n times (calculate F(r) in all those times), we are sure that the intervals (1-u, u) jointly contain Y=F(r), 1-alpha of the time
uhat <- quantile(Ustar, probs = 1 - alpha)
uhat
```


```{r}
# definition of quantile
length(U[U<= 0.9931013])/length(U)
```


8. The join confidence region is calculated

```{r results='asis'}
# library("GoFKernel")
# 
# F.inv <- lapply(estimated_CDF, function(F){inverse(F, lower = -100, upper = 100)})
# dataset['F.inv_u'] <- sapply(1:K, function(i){F.inv[[i]](uhat)})
# dataset['F.inv_1-u'] <- sapply(1:K, function(i){F.inv[[i]](1-uhat)})
# 
# # sorted_theta_k <- sort(dataset$theta_k)
# dataset['kde_ci_lower'] <- dataset['theta_k'] - dataset['F.inv_u']
# dataset['kde_ci_upper'] <- dataset['theta_k'] - dataset['F.inv_1-u']
# 
# interval_table <- dataset %>%
#     select(rank = k, estimate = theta_k, lower = kde_ci_lower, upper = kde_ci_upper)
# kable(interval_table) %>% kable_styling(bootstrap_options = "responsive")

library("GoFKernel")

sorted_indices <- order(dataset$theta_k)
sorted_dataset <- dataset[sorted_indices, ]  # for safe CI computation
sorted_theta_k <- sorted_dataset$theta_k

Fhat.inv <- lapply(Fhat, function(F) inverse(F, lower = -100, upper = 100))

sorted_dataset$Fhat.inv_u    <- sapply(1:K, function(i) Fhat.inv[[i]](uhat))
sorted_dataset$Fhat.inv_1_u  <- sapply(1:K, function(i) Fhat.inv[[i]](1 - uhat))

sorted_dataset$kde_ci_lower <- sorted_theta_k - sorted_dataset$Fhat.inv_u
sorted_dataset$kde_ci_upper <- sorted_theta_k - sorted_dataset$Fhat.inv_1_u

sorted_dataset$original_index <- sorted_indices

interval_table <- sorted_dataset %>%
  select(k = k,
         theta_k = theta_k,
         kde_ci_lower = kde_ci_lower,
         kde_ci_upper = kde_ci_upper)

kable(interval_table, digits = 4) %>%
  kable_styling(bootstrap_options = "responsive")
```

```{r echo=FALSE, eval =FALSE}

within_ci_transformed <- function(Y, uhat){1-uhat < Y & Y < uhat}
sum(rowSums(apply(Y,c(1,2), function(Y)within_ci_transformed(Y, uhat))) == 10)/B

Finv_u <- sapply(F.inv, function(x)x(uhat))
Finv_1_u <- sapply(F.inv, function(x)x(1-uhat))
sorted_thetahat <- sort(dataset$theta_k)

#(sorted_thetahat - Finv_u < theta) & (theta <= sorted_thetahat - Finv_1_u)

#mean(sapply(1:B, function(i)sum((mat1_sorted[i,] - Finv_u < theta) & (theta <- mat1_sorted[i,] - Finv_1_u)))==K)
```

### Full function

#### Sample usage

```{r}
source('../R/algo1_helper.R')

alpha <- 0.10
K <- 10
B <- 1000
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")
# filter accdg to K
dataset <- df[order(df$k),][1:K,]
# run for K = 10
output_for_k10 <- run_algorithm1(B, dataset, seed = 4, alpha = 0.1)
kable(output_for_k10, row.names = FALSE) %>% kable_styling(bootstrap_options = "responsive")
```



```{r echo = FALSE}
# dataset <- df[order(df$k),][1:30,]
# B=1000
# 
# run_algorithm1 <- function(B, dataset, seed = 4) {
#   dataset['variance'] <- lapply(dataset['moe_k'], calculate_variance)
#   K <- dim(dataset)[1]
#   set.seed(seed)
#   mat1 <- foreach(i = 1:K, .combine = cbind) %do% {
#     foreach(b = 1:B, .combine = c) %do% {
#       rnorm(1, mean = dataset[i, 'theta_k'], sd = sqrt(dataset[i, 'variance']))
#     }
#   }
#   # 2
#   mat1_sorted <- t(apply(mat1, 1, sort))
#   # 3
#   r <- mat1_sorted - matrix(sort(t(dataset['theta_k'])),B, K, byrow=TRUE)
#   # 4
#   estimated_CDF <- lapply(1:K, function(x)(estimate_CDF(dataset, x, r)))
#   # 5
#   Y <- foreach(i = 1:K, .combine = cbind) %do% {
#     sapply(r[,i], estimated_CDF[[i]])
#   }
#   # 6
#   U <- apply(apply(Y, MARGIN = c(1, 2), FUN = get_inner_max), 1, max)
#   # 7
#   uhat <- quantile(U, probs = 1 - alpha)
#   # 8
#   
#   F.inv <- lapply(estimated_CDF, function(F){inverse(F, lower = -100, upper = 100)})
#   dataset['F.inv_u'] <- sapply(1:K, function(i){F.inv[[i]](uhat)})
#   dataset['F.inv_1-u'] <- sapply(1:K, function(i){F.inv[[i]](1-uhat)})
#   dataset['kde_ci_lower'] <- dataset['theta_k'] - dataset['F.inv_u'] 
#   dataset['kde_ci_upper'] <- dataset['theta_k'] - dataset['F.inv_1-u']
#   return(dataset[, c('rhat_k', 'k', 'theta_k', 'kde_ci_lower', 'kde_ci_upper')])
# }
```

#### Application to diff K

```{r eval = FALSE}
B<-1000
for (K in c(10, 20, 30, 40, 51)) {
  # filtering the dataset
  dataset <- df[order(df$k),][1:K,]
  # running the algo1
  result <- run_algorithm1(B, dataset)$interval_table
  # saving the file
  filename <- paste0("../data/kde_intervals_", K, ".rds")
  saveRDS(result, file = filename)
}
```


```{r echo = FALSE}
# get_ranks <- function(k, tuple_list){
#   Lambda_lk <- which(tuple_list[,2]<=tuple_list[k,1])
#   Lambda_lk <- Lambda_lk[Lambda_lk != k]
#   Lambda_Ok <- which(tuple_list[,2]>tuple_list[k,1] & tuple_list[k,2] > tuple_list[,1])
#   Lambda_Ok <- Lambda_Ok[Lambda_Ok != k]
# 
#   ranks <- seq(
#     length(unique(Lambda_lk)) + 1,
#     length(unique(Lambda_lk)) + length(unique(Lambda_Ok)) + 1,
#     1
#   )
#   
#   return(list(
#     ranks = ranks,
#     Lambda_Ok = Lambda_Ok
#   ))
# }
```


```{r results='asis'}
output <- list()

# original columns: "rhat_k","k","theta_k","kde_ci_lower,kde_ci_upper"
# the rest are added on this chunk

for (K in c(10, 20, 30, 40, 51)) {
  # reading the outputs saved previously
  filename <- paste0("../data/kde_intervals_", K, ".rds")
  output[[K]] <- readRDS(filename)
  
  # converting into tuples
  tuple_list <- t(apply(output[[K]][,c('kde_ci_lower', 'kde_ci_upper')], 1, function(row) as.numeric(row)))
  
  # calculating the min from ranks
  output[[K]]['kde_rank_lower'] <- sapply(1:K, function(x) min(get_ranks(x, tuple_list)$ranks))
  
  # calculating the max from ranks
  output[[K]]['kde_rank_upper'] <- sapply(1:K, function(x) max(get_ranks(x, tuple_list)$ranks))
  
  # cardinality based on lambdaOK
  output[[K]]['kde_Lambda_Ok_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$Lambda_Ok))
  
  # cardinality based on computed ranks
  output[[K]]['kde_rank_range_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$ranks))
  print(paste("**** Output shown for K=", K, "****"))
  
  # adding something similar to rhatk only for K=10
  output[[K]]['sample_rank'] <- rank(output[[K]]$theta_k, ties.method = "max")
  print(kable(output[[K]], row.names = FALSE) %>% kable_styling(bootstrap_options = "responsive"))
  cat("\n\n---\n\n")
}
```



### Plots

#### KDE

```{r fig.height = 8, fig.width=10, echo = FALSE}
kde_result <- output[[51]]
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")
df$order_index <- seq(K, 1, -1)

kde_result <- merge(x = df[,c('k', 'iso', 'order_index')], y = kde_result, by = "k", all.x = TRUE)

library(ggplot2)
library(dplyr)
library(tidyr)

to_string_seq <- function(x, y){paste(seq(x, y, 1), collapse=',')}

dat_to_plot <- kde_result %>% 
    select(c(k, iso, kde_rank_lower, kde_rank_upper, sample_rank, order_index,theta_k)) %>%
    mutate(string_ranks = mapply(to_string_seq, kde_rank_lower, kde_rank_upper)) %>%
    separate_rows(string_ranks, sep=",") %>%
    select(c(k, iso, sample_rank, string_ranks, theta_k))

dat_to_plot$highlight <- ifelse(dat_to_plot$sample_rank == as.numeric(dat_to_plot$string_ranks), "highlight", "normal")

p <- ggplot(data = dat_to_plot, aes(x = reorder(k, theta_k),
                               y = as.numeric(string_ranks),
                               label = iso,
                               color = highlight)) + geom_text(size = 2) +
  scale_color_manual(values = c("highlight" = "red", "normal" = "black")) +
  scale_y_continuous(breaks = 1:51) +
  labs(title='(KDE) visualization of the 90% joint confidence') + xlab('States') + ylab('Sample rank') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        ) +
  guides(colour=FALSE) +
    theme(axis.line = element_line(colour = "gray"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),)

#ggsave("../figures/kde_ranks_plot.png", plot = p, width = 10, height = 8, dpi = 300)
p
```

#### IND

```{r fig.height = 8, fig.width=10, echo = FALSE}
to_string_seq <- function(x, y){paste(seq(x, y, 1), collapse=',')}

dat_to_plot <- df %>% 
    select(c(k, iso, ind_rank_lower, ind_rank_upper, rhat_k, order_index, theta_k)) %>%
    mutate(string_ranks = mapply(to_string_seq, ind_rank_lower, ind_rank_upper)) %>%
    separate_rows(string_ranks, sep=",") %>%
    select(c(k, iso, rhat_k, string_ranks, order_index, theta_k))

dat_to_plot$highlight <- ifelse(dat_to_plot$rhat_k == as.numeric(dat_to_plot$string_ranks), "highlight", "normal")

p <- ggplot(data = dat_to_plot, aes(x = reorder(k, theta_k),
                               y = as.numeric(string_ranks),
                               label = iso,
                               color = highlight)) + geom_text(size = 2) +
  scale_color_manual(values = c("highlight" = "red", "normal" = "black")) +
  scale_y_continuous(breaks = 1:51) +
  labs(title='(IND) visualization of the 90% joint confidence') + xlab('States') + ylab('Sample rank') +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        ) +
  guides(colour=FALSE) +
    theme(axis.line = element_line(colour = "gray"),
    panel.grid.minor = element_blank(),
    panel.grid.major.x = element_blank(),)

#ggsave("../figures/ind_ranks_plot.png", plot = p, width = 10, height = 8, dpi = 300)
p
```

## T1, T2, T3


```{r}
tuple_list <- t(apply(df[,c('bonf_ci_lower', 'bonf_ci_upper')], 1, function(row) as.numeric(row)))
df['bonf_Lambda_Ok_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$Lambda_Ok))
df['bonf_rank_range_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$ranks))

tuple_list <- t(apply(df[,c('ind_ci_lower', 'ind_ci_upper')], 1, function(row) as.numeric(row)))
df['ind_Lambda_Ok_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$Lambda_Ok))
df['ind_rank_range_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$ranks))

```


```{r results='asis'}
#Oks <- list(df$bonf_Lambda_Ok_length, df$ind_Lambda_Ok_length, output[[51]]$kde_Lambda_Ok_length)
Oks <- list(df$bonf_rank_range_length, df$ind_rank_range_length, output[[51]]$kde_rank_range_length)

kable(data.frame(
  T1 = sapply(Oks, get_t1),
  T2 = sapply(Oks, get_t2),
  T3 = sapply(Oks, get_t3), row.names = c("bonf", "ind", "kde"))) %>% kable_styling(bootstrap_options = "responsive")
```

## Algorithm 2


```{r eval = FALSE}
source('../R/algo1_helper.R')
library(doParallel)
library(arrow)
library(tictoc)

K <- 10
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")
dataset_or <- df[order(df$k),][1:K,]

cl=parallel::makeCluster(15)
registerDoParallel(cl)

get_coverage <- function(dataset_or, K, reps = 100, B=1000, alpha= 0.10){
  foreach(iter = 1:reps, .combine = rbind,.packages = c("foreach", "arrow", "dplyr"),
          .export = c("run_algorithm1", 
                     # 'calculate_variance', 
                      "estimate_CDF",
                      "get_ranks", "get_t2", "kde_cdf_function",'get_inner_max',
                      "inverse")) %dopar% {

  dataset <- dataset_or
  set.seed(iter)
  theta_hat <- foreach(i = 1:K, .combine = cbind) %do% {
      rnorm(1, mean = dataset[i, 'theta_k'], sd = dataset[i, 'S'])
  }
  sort_reference <- data.frame(theta = dataset$theta_k, theta_hat = t(theta_hat))
  colnames(dataset)[colnames(dataset) == 'theta_k'] <- 'otheta_k'


  dataset$theta_k <- t(theta_hat) #sampled
  result <- run_algorithm1(B, dataset, iter, alpha)
  
  #sorted_theta <- sort(sort_reference$theta) #true value
  sorted_indices <- result$sorted_indices
  sorted_theta <- sort_reference$theta[sorted_indices]
  sorted_thetahat <- sort_reference$theta_hat[sorted_indices]
  
  # inside_check <- sum((result$sorted_thetahat - result$Finv_u < sorted_theta) & (sorted_theta < result$sorted_thetahat - result$Finv_1_u))
  
  inside_check <- sum((sorted_thetahat - result$Finv_u < sorted_theta) & 
                    (sorted_theta < sorted_thetahat - result$Finv_1_u))
  
  Fhat <- result$Fhat
  uhat <- result$uhat
  #rstar <- result$sorted_thetahat - sorted_theta
  rstar <- sorted_thetahat - sorted_theta
  
  Ystar <- sapply(1:length(Fhat), function(i) {
    Fhat[[i]](rstar[i])
  })
  
  inside_check_primary <-  sum(((1-uhat) < Ystar) & ((Ystar) < uhat))
  
  tab_result <- result$interval_table
  tuple_list <- t(apply(tab_result[,c('kde_ci_lower', 'kde_ci_upper')], 1, function(row) as.numeric(row)))
  
  tab_result['ind_rank_range_length'] <- sapply(1:K, function(x) length(get_ranks(x, tuple_list)$ranks))
  
  write_feather(tab_result, sprintf("../output/iters/K%d/sorted_ci_%04d.feather", K, iter))
  
  t2_val <- get_t2(tab_result$ind_rank_range_length)
  
  inside_check_table <- sum((sorted_theta > tab_result$kde_ci_lower) & (sorted_theta < tab_result$kde_ci_upper))
  
  data.frame(t2 = t2_val, inside = inside_check, inside_primary = inside_check_primary, inside_check_table = inside_check_table)
}
}
```


```{r eval = FALSE}
library(doParallel)
library(foreach)
library(tictoc)

df <- readRDS("../data/mean_travel_time_ranking_2011.rds")
cl=parallel::makeCluster(15)
registerDoParallel(cl)
for (K in c(3,4,5,6,7,8,9,10,15,20,25,30,35,40,45,51)) {
  set.seed(4)
  dataset_or <- df[order(df$k), ][1:K, ] #df[order(df$k),][1:K,]
  #set.seed(5)
  #dataset_or$S <- runif(K, 0.07, 0.07) #0.37

  tic("Running...")
  
  
  coverage_output_df <- get_coverage(dataset_or, K, reps = 100, B=1000, alpha=0.1)
  toc()
  
  saveRDS(coverage_output_df,  paste0("../output/coverage_probability_",K, ".rds"))
  coverage_output_df
  
}
stopCluster(cl)

# Running...: 16.932 sec elapsed
# Running...: 17.73 sec elapsed
# Running...: 23.867 sec elapsed
# Running...: 27.409 sec elapsed
# Running...: 30.998 sec elapsed
# Running...: 35.408 sec elapsed
# Running...: 39.138 sec elapsed
# Running...: 42.961 sec elapsed
# Running...: 84.71 sec elapsed
# Running...: 127.815 sec elapsed
# Running...: 168.727 sec elapsed
# Running...: 173.434 sec elapsed
# Running...: 230.43 sec elapsed
```


### Iteration samples {.hidden .unlisted .unnumbered}


Aim is to observe which states usually fail to make it within the estimated interval, making the coverage low, even if it is just one miss. It appears that states with higher SD usually fail to make it within the interval. Plots below show misses as black points. They are below the y = 0 line if they are closer to the lower bound and above it otherwise. 

K = 10

```{r echo = FALSE, results = "asis", fig.width=9, eval = FALSE}
library(arrow)
library(ggplot2)
source('../R/algo2_helper.R')
K = 10
file_list <- list.files(path = paste0("../output/iters/K",K), pattern = "\\.feather$", full.names = TRUE)
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")

dataset_or <- df[order(df$k),][1:K,]

sorted_theta <- sort(dataset_or$theta_k)
misses <- get_iter_misses(K=K, file_list=file_list, sorted_theta=sorted_theta)

misses$diff1 <- misses$theta_k - misses$kde_ci_lower
misses$diff2 <- misses$theta_k - misses$kde_ci_upper

misses$closest_diff <- ifelse(abs(misses$diff1)<abs(misses$diff2), misses$diff1, 1*misses$diff2)
misses$closest_diff <- ifelse(!misses$theta_within, misses$closest_diff, 0)
misses <- misses %>% left_join(df[, c('k', 'S')], by = "k")

misses %>%
  ggplot(aes(x = reorder(k, S), y = closest_diff)) +
  geom_point(aes(color = closest_diff == 0, size = S), alpha = 0.2) +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(color = "On Target?") + xlab("State Arranged in Increasing S") + ylab("Difference from closest bound") + scale_size_continuous(trans = "sqrt", range = c(0, 3)) + guides(size = FALSE)
```

K = 20 

```{r echo = FALSE, results = "asis", fig.width=9, eval = FALSE}
K<-20
file_list <- list.files(path = paste0("../output/iters/K",K), pattern = "\\.feather$", full.names = TRUE)
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")

dataset_or <- df[order(df$k),][1:K,]

sorted_theta <- sort(dataset$otheta_k)
misses <- get_iter_misses(K=K, file_list=file_list, sorted_theta=sorted_theta)

misses$diff1 <- misses$estimate - misses$lower
misses$diff2 <- misses$estimate - misses$upper

misses$closest_diff <- ifelse(abs(misses$diff1)<abs(misses$diff2), misses$diff1, 1*misses$diff2)
misses$closest_diff <- ifelse(!misses$theta_within, misses$closest_diff, 0)
misses <- merge(misses, df[, c('k', 'S')], by.x = "rank", by.y = "k")

misses %>%
  ggplot(aes(x = reorder(rank, S), y = closest_diff)) +
  geom_point(aes(color = closest_diff == 0, size = S), alpha = 0.2) +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(color = "On Target?") + xlab("State Arranged in Increasing S") + ylab("Difference from closest bound") + scale_size_continuous(trans = "sqrt", range = c(0, 3)) + guides(size = FALSE)
```

K = 30


```{r echo = FALSE, results = "asis", fig.width=9, eval = FALSE}
K<-30
file_list <- list.files(path = paste0("../output/iters/K",K), pattern = "\\.feather$", full.names = TRUE)
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")

dataset_or <- df[order(df$k),][1:K,]

sorted_theta <- sort(dataset$otheta_k)
misses <- get_iter_misses(K=K, file_list=file_list, sorted_theta=sorted_theta)

misses$diff1 <- misses$estimate - misses$lower
misses$diff2 <- misses$estimate - misses$upper

misses$closest_diff <- ifelse(abs(misses$diff1)<abs(misses$diff2), misses$diff1, 1*misses$diff2)
misses$closest_diff <- ifelse(!misses$theta_within, misses$closest_diff, 0)
misses <- merge(misses, df[, c('k', 'S')], by.x = "rank", by.y = "k")

misses %>%
  ggplot(aes(x = reorder(rank, S), y = closest_diff)) +
  geom_point(aes(color = closest_diff == 0, size = S), alpha = 0.2) +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(color = "On Target?") + xlab("State Arranged in Increasing S") + ylab("Difference from closest bound") + scale_size_continuous(trans = "sqrt", range = c(0, 3)) + guides(size = FALSE)
```
K = 40

```{r echo = FALSE, results = "asis", fig.width=9, eval = FALSE}
K <- 10
file_list <- list.files(path = paste0("../output/iters/K", K), pattern = "\\.feather$", full.names = TRUE)
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")

# Extract true parameters from original dataset, sorted by true theta (not by rank)
dataset_or <- df[order(df$k), ][1:K, ]
names(dataset_or)[names(dataset_or) == "theta_k"] <- "otheta_k"

# Create mapping: rank -> true theta (already sorted by rank in dataset_or)
sorted_theta <- dataset_or$otheta_k  # already aligned with rank

# Get misses
misses <- get_iter_misses(K = K, file_list = file_list, sorted_theta = sorted_theta)

# Compute distance to nearest interval bound
misses$diff1 <- misses$estimate - misses$lower
misses$diff2 <- misses$estimate - misses$upper
misses$theta_within <- !misses$theta_within

misses$closest_diff <- ifelse(abs(misses$diff1) < abs(misses$diff2), misses$diff1, misses$diff2)
misses$closest_diff <- ifelse(!misses$theta_within, misses$closest_diff, 0)

# Merge standard error S by rank
misses <- merge(misses, df[, c('k', 'S')], by.x = "rank", by.y = "k")

# Plot
misses %>%
  ggplot(aes(x = reorder(factor(rank), S), y = closest_diff)) +
  geom_point(aes(color = closest_diff == 0, size = S), alpha = 0.4) +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    color = "On Target?",
    x = "State Arranged in Increasing S",
    y = "Difference from Closest Bound"
  ) +
  scale_size_continuous(trans = "sqrt", range = c(0, 3)) +
  guides(size = FALSE)

```

K = 51


```{r echo = FALSE, results = "asis", fig.width=9, eval = FALSE}
K<-51
file_list <- list.files(path = paste0("../output/iters/K",K), pattern = "\\.feather$", full.names = TRUE)
df <- readRDS("../data/mean_travel_time_ranking_2011.rds")

dataset_or <- df[order(df$k),][1:K,]

sorted_theta <- sort(dataset$otheta_k)
misses <- get_iter_misses(K=K, file_list=file_list, sorted_theta=sorted_theta)

misses$diff1 <- misses$estimate - misses$lower
misses$diff2 <- misses$estimate - misses$upper

misses$closest_diff <- ifelse(abs(misses$diff1)<abs(misses$diff2), misses$diff1, 1*misses$diff2)
misses$closest_diff <- ifelse(!misses$theta_within, misses$closest_diff, 0)
misses <- merge(misses, df[, c('k', 'S')], by.x = "rank", by.y = "k")

misses %>%
  ggplot(aes(x = reorder(rank, S), y = closest_diff)) +
  geom_point(aes(color = closest_diff == 0, size = S), alpha = 0.2) +
  scale_color_manual(values = c("FALSE" = "black", "TRUE" = "red")) +
  theme_bw() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(color = "On Target?") + xlab("State Arranged in Increasing S") + ylab("Difference from closest bound") + scale_size_continuous(trans = "sqrt", range = c(0, 3)) + guides(size = FALSE)
```

### Results from **100** reps

```{r}
library(dplyr)
# Ks <- c(51, 40, 30, 20, 10)
Ks <- c(51,45,40,35,30,25,20,15,10,9,8,7,6,5,4,3)

results <- lapply(Ks, function(K) {
  a <- readRDS(paste0("../output/coverage_probability_", K, ".rds"))
  #print(a)
  data.frame(
    K = K,
    Coverage = mean(a$inside == K),  # only TRUE if inside == K
    Within_CI_mean = mean(a$inside),
    Within_CI_max = max(a$inside),
    Within_CI_min = min(a$inside),
    T_mean = mean(a$t2)
  )
})

df <- do.call(rbind, results)

#kable(df) %>% kable_styling(bootstrap_options = "responsive")
df
write.csv(df, "constant_variance_point7_result.csv", row.names = FALSE)
```

### Breakdown of within CI per K

```{r}
Ks <- c(51, 40, 30, 20, 10,5)

results <- lapply(Ks, function(K) {
  a <- readRDS(paste0("../output/coverage_probability_", K, ".rds"))
  data.frame(K = K, inside = a$inside)
})

df_all <- do.call(rbind, results)

summary_counts <- df_all %>%
  group_by(K, inside) %>% summarise(count = n(), .groups = "drop_last") %>% mutate(proportion = count / sum(count)) %>%
  ungroup()

summary_counts %>%
  kable(align = "c") %>%
  kable_styling(bootstrap_options = "striped", full_width = FALSE) %>%
  row_spec(which(summary_counts$K == summary_counts$inside), background = "lightyellow")


```

### Experimental runs results

K = 51, 100 reps, alpha = 0.01, cov = 0.19  
order in dec S, K = 10, alpha = 0.1, rep 100, cov = 0.75  
order in inc S, K = 10, alpha = 0.1, rep 100, cov = 0.79  

### Experimental runs results - to redo {.hidden .unlisted .unnumbered}

#### alpha

K = 10, 100 reps
alpha = 0.05, B=1000: cov 0.2857143  
alpha = 0.01, B=1000: cov 0.44  
alpha = 0.01, B=2000: cov 0.43  
alpha = 0.0005, B=1000: cov 0.67  
alpha = 0.0001, B=1000: cov 0.67  
alpha = 0.00001, B=1000: cov 0.68  

#### SD

Notes:  
0.14 0.33 0.15 0.23 0.07 0.19 0.19 0.37 0.32 0.11  
original SDs: 0.23  

SD set to 0.2 (fixed), coverage: 0.77  
SD set to rnorm(n, 0.2, 0.001), coverage: 0.77  
SD set to rnorm(n, 0.2, 0.01), coverage: 0.77  
SD set to rnorm(n, 0.2, 0.1), coverage: 0.39  

SD set to 0.1 (fixed), coverage: 0.86  
SD set to rnorm(n, 0.1, 0.001), coverage: 0.86  
SD set to rnorm(n, 0.1, 0.01), coverage: 0.83  


SD set to 0.5 (fixed), coverage: 0.52  
SD set to rnorm(n, 0.5, 0.001), coverage: 0.52  
SD set to rnorm(n, 0.5, 0.01), coverage: 0.5  
SD set to rnorm(n, 0.5, 0.1), coverage: 0.43  

0.14 0.33 0.15 0.23 0.07 0.19 0.19 0.37 0.32 0.11  
original SDs: 0.23  

SD set to runif(dim(df)[1], 0.1, 0.6): 0.24  
SD set to runif(dim(df)[1], 0.1, 0.5): 0.28 <-  min max of K=51 is used  
SD set to runif(dim(df)[1], 0.07, 0.37): 0.28 <-  min max of K=10 is used  
SD set to runif(dim(df)[1], 0.1, 0.4): 0.33  
SD set to runif(dim(df)[1], 0.1, 0.3): 0.41  
SD set to runif(dim(df)[1], 0.1, 0.2): 0.55  

SD set to 0.1 (fixed), coverage: 0.86 <- fixed higher  
SD set to 0.05 (fixed), coverage: 0.91 <- fixed higher  
SD set to runif(dim(df)[1], 0.05, 0.06): 0.87 <- narrower  
SD set to runif(dim(df)[1], 0.05, 0.1): 0.64 <- broader  

Seems like when SD across varies highly across items to rank, leads to lower coverage, besides the extremeness of the SD themselves  

# Appendices

## Kernel Density Estimation

This is from @mariarizzo.

Let

```{r}
# Generate data
set.seed(123)
x_data <- rnorm(100, mean = 3, sd = 1)
# Parameters
x_point <- 3
h <- 0.5 # arbitrary
n <- length(x_data)
```

If a histogram with bin width $h$ is constructed from a sample $X_1, \dots, X_n$, then a density estimate for a point $x$ within the range of the data is

$$
\hat{f}(x) = \frac{1}{2hn} \times k
$$

where $k$ is the number of sample points in the interval $\left( x-h, x+h\right)$. This estimator can be written as

$$
\hat{f}(x) = \frac{1}{n} \sum^n_{i=1} \frac{1}{h}w \left( \frac{x - X_i}{h} \right) (\#eq:nde)
$$

where $w(t) =  \frac{1}{2} I(|t| < 1)$ is a weight function. The density estimator $\hat{f}(x)$ in \@ref(eq:nde) with $w(t) =  \frac{1}{2} I(|t| < 1)$ is called naive density estimator. This weight function has the property that $\int^{\infty}_{-\infty} K(t) dt = 1$, and $w(t) \geq 0$, so $w(t)$ is a probability density supported on the interval $[-1, 1]$.

```{r}
# weight function (naive)
naive_kernel <- function(t) {
  if (abs(t) < 1) return(0.5)
  else return(0)
}

# use the weight function to compute weight for each data point
# you check if t, not your raw point, is within the -1 to 1 window
# x_data is Xi in the formula
t <- (x_point - x_data) / h
weights <- sapply(t, naive_kernel)


# naive density estimate for a point x
f_hat <- (1 / (n * h)) * sum(weights)
print(f_hat)
```

```{r}
weights
```


```{r}
# pts inside the window
in_window <- x_data[abs((x_point - x_data) / h) < 1]
hist(x_data, breaks = 20, col = "gray90", border = "gray40",
     freq = FALSE, main = "Naive KDE Estimate at x = 3", xlab = "x")
abline(v = x_point, col = "blue", lty = 2, lwd = 2)              # Point of estimate
points(x_point, f_hat, col = "red", pch = 19, cex = 1.5)
text(x_point, f_hat, labels = round(f_hat, 3), pos = 3, col = "red")
```

```{r fig.height=4}

# Naive KDE function
naive_kde <- function(x, x_data, h) {
  n <- length(x_data)
  weights <- sapply(x_data, function(Xi) {
    t <- (x - Xi) / h
    naive_kernel(t)
  })
  return((1 / (n * h)) * sum(weights))
}

# KDE on a grid: take the KDEstimate for each point in the data
x_grid <- seq(min(x_data) - 1, max(x_data) + 1, length.out = 200)
kde_vals <- sapply(x_grid, naive_kde, x_data = x_data, h = h)


hist(x_data, breaks = 20, col = "gray90", border = "gray40",
     freq = FALSE, main = "Naive KDE vs R::density()",
     xlab = "x", xlim = range(x_grid), ylim = c(0, max(kde_vals)*1.5))

# Overlay manual KDE
lines(x_grid, kde_vals, col = "blue", lwd = 2)

# Overlay R's built-in KDE with rectangular kernel
lines(density(x_data, bw = h, kernel = "rectangular"), col = "red", lwd = 2, lty = 2)
rug(x_data, col = "gray40")
legend("topright", legend = c("Naive KDE (manual)", "R: density() with rectangular kernel"),
       col = c("blue", "red"), lty = c(1, 2), lwd = 2, cex = 0.5)

```

::: {style="border-left: 4px solid #007ACC; padding: 1em; background: #f9f9f9;"}
**Aside:**\
This function is 1 when $t$ is between $-1$ and $1$, and $0$ outside that range.

\begin{align*}
I(|t|<1) = 
\begin{cases}
  1 & \text{if} |t| < 1 \\
  0 & \text{otherwise}
\end{cases}
\end{align*}

This weight function is $\frac{1}{2}$ when $t$ is between $-1$ and $1$, and $0$ elsewhere.

$$
w(t) =  \frac{1}{2} I(|t| < 1)
$$

The below illustration of the kernel shows it is a valid PDF with area under the curve 1.

```{r echo = FALSE}
# Create a sequence of t values from -3 to 3
t <- seq(-3, 3, length.out = 500)

# Define the weight function w(t)
w <- ifelse(abs(t) < 1, 0.5, 0)

plot(t, w, type = "l", lwd = 1, col = "blue",
     xlab = "t", ylab = "w(t)",
     main = "Naive Kernel Function\n w(t) = 0.5 if |t| < 1, else 0")
abline(v = -1, col = "red", lty = 2)
abline(v = 1, col = "red", lty = 2)

```
:::

Kernel density estimation replaces the weight function $w(t)$ in the naive estimator with a function $K(\cdot)$ called a kernel function such that.

$$
\int^{\infty}_{-\infty} K(t) dt = 1
$$

In probability density estimation, $K(\cdot)$ is usually a symmetric probability density function. The weight function $w(t)=\frac{1}{2} I(|t| < 1)$ is called the rectangular kernel. The rectangular kernel is a symmetric probability density centered at the origin, and

$$
\frac{1}{nh} w \left(\frac{x-X_i}{h} \right)
$$ corresponds to a rectangle of area $\frac{1}{n}$ centered at $X_i$. The density estimate at $x$ is the sum of rectangles located within $h$ units from $x$.

Restricting our attention to symmetric positive kernel density estimators, suppose that $K(\cdot)$ is another symmetric probability density centered at the origin, and define

$$
\hat{f}_K(x) = \frac{1}{n} \sum^n_{i=1} \frac{1}{h} K \left( \frac{x-X_i}{h} \right) (\#eq:spd)
$$

The, $\hat{f}$ is a probability density function. For example, $K(x)$ may be the triangular density on $[−1, 1]$ (the triangular kernel) or the standard normal density (the Gaussian kernel). The triangular kernel estimator corresponds to the sum of areas of triangles instead of rectangles. The Gaussian kernel estimator centers a normal density at each data point. The histogram density estimator corresponds to the rectangular kernel density estimator.

The bin width $h$ is a smoothing parameter; small values of $h$ reveal local features of the density, while large values of $h$ produce a smoother density estimate. In kernel density estimation $h$ is called the bandwidth, smoothing parameter or window width.

```{r echo=FALSE, eval = FALSE}
library(shiny)

# Generate sample data
set.seed(123)
x_data <-rexp(1000, 1)# c(-0.77, -0.60, -0.25, 0.14, 0.45, 0.64, 0.65, 1.19, 1.71, 1.74)
sliderInput("h", "Bandwidth (h):", min = 0.1, max = 2, value = 0.4, step = 0.05)
selectInput("kernel", "Kernel:", choices = c("gaussian","epanechnikov", "rectangular", "triangular", "biweight", "optcosine"), selected = "gaussian")


renderPlot({
  h <- input$h
  kernel_type <- input$kernel
  n <- length(x_data)
  x_grid <- seq(min(x_data) - 1, max(x_data) + 1, length.out = 300)
  
  kernel_variance <- switch(kernel_type,
  "gaussian" = 1,
  "epanechnikov" = 1/5,
  "rectangular"  = 1/3,
  "triangular" = 1/6,
  "biweight" = 1/7,
  "optcosine" = 1-8/pi^2
)

  variance_scaling <- sqrt(kernel_variance)
  adjusted_h <- h / variance_scaling 


  ## Normalize a kernel function to integrate to 1 over its support
  # normalize_kernel <- function(f, lower = -1, upper = 1) {
  #   norm_const <- integrate(f, lower, upper)$value
  #   function(t) f(t) / norm_const
  # }

  # Raw kernel functions
  raw_kernel_function <- switch(kernel_type,
    "gaussian" = function(t) (1 / sqrt(2 * pi)) * exp(-0.5 * t^2),
    "epanechnikov" = function(t) ifelse(abs(t) <= 1, (3/4)*(1 - t^2), 0),
    "rectangular"  = function(t) ifelse(abs(t) <= 1, 1/2, 0),
    "triangular" = function(t) ifelse(abs(t) <= 1, 1 - abs(t), 0),
    "biweight" = function(t) ifelse(abs(t) <= 1, (15/16)*(1 - t^2)^2, 0),
    "optcosine" = function(t) ifelse(abs(t) <= 1, (pi/4)*cos((pi*t)/2),0)
  )

  # Integration limits (Gaussian is over whole line)
  # integration_limits <- if (kernel_type == "gaussian") c(-Inf, Inf) else c(-1, 1)

  # Normalize the kernel function
  kernel_function <- raw_kernel_function
  # normalize_kernel(
  #   raw_kernel_function,
  #   lower = integration_limits[1],
  #   upper = integration_limits[2]
  # )

  # Manual KDE
  kde_vals <- sapply(x_grid, function(x) {
    sum(sapply(x_data, function(Xi) {
      
      t <- (x - Xi) / adjusted_h
      
      kernel_function(t) / adjusted_h
    })) / n
  })

  # Plot setup
  plot(x_grid, kde_vals, type = "n", xlab = "x", ylab = "Density",
       main = paste("Kernel:", kernel_type, " | Bandwidth:", round(h, 2)),
       ylim = c(0, max(kde_vals)*1.2))

  hist(x_data, probability = TRUE, col = rgb(0.8, 0.8, 0.8, 0.4),
     border = "white", add = TRUE, breaks = "FD")  # or use breaks = 10
  
  # add the true density to compare
  y <- seq(.001, 6, .01)
  lines(y, dexp(y, 1), lty = 2)
  
  # Individual kernel bumps
  for (Xi in x_data) {
    bump_x <- seq(Xi - 3*h, Xi + 3*h, length.out = 100)
    bump_y <- sapply(bump_x, function(x) {
      t <- (x - Xi) / adjusted_h
      kernel_function(t) / (adjusted_h * n)
    })
    lines(bump_x, bump_y, col = rgb(0.2, 0.2, 0.8, 0.3))
  }

  # KDE curve
  lines(x_grid, kde_vals, col = "blue", lwd = 2)

  # R's density function for comparison
  r_dens <- density(
    x_data, bw = h, kernel = kernel_type,
    n = length(x_grid), from = min(x_grid), to = max(x_grid)
  )
  lines(r_dens$x, r_dens$y, col = "red", lwd = 2, lty = 2)

  rug(x_data, col = "gray40")
})

```


```{r echo = FALSE, eval = FALSE}
library(MASS)
radioButtons(
  "hs",
  "Bandwidth (h):",
  choices = c('3.998', '2','5','7'),
  selected = '3.998', inline = TRUE
)


waiting <- geyser$waiting
n <- length(waiting)
h1 <- 1.06 * sd(waiting) * n^(-1/5)
h2 <- .9 * min(c(IQR(waiting)/1.34, sd(waiting))) * n^(-1/5)

renderPlot({
  h_val <- as.numeric(input$hs)
  plot(density(waiting, bw=h_val), main = paste("Geyser Waiting Data: Bandwidth(",h_val, ")"))
})

```



... checking `get_ranks` fcn is aligned to handout example

```{r}
df<-data.frame(lower = c(2, 4, 8, 9), upper = c(6, 7, 11, 10))
tuple_list <-  t(apply(df[,c('lower', 'upper')], 1, function(row) as.numeric(row)))
```


```{r}
get_ranks(1, tuple_list)
get_ranks(2, tuple_list)
get_ranks(3, tuple_list)
get_ranks(4, tuple_list)

```

```{r eval = FALSE}
# function check
tuple_list <- t(apply(df[,c('bonf_ci_lower', 'bonf_ci_upper')], 1, function(row) as.numeric(row)))
df$bonf_rank_lower == sapply(1:K, function(x) min(get_ranks(x, tuple_list)$ranks))
df$bonf_rank_upper == sapply(1:K, function(x) max(get_ranks(x, tuple_list)$ranks))


tuple_list <- t(apply(df[,c('ind_ci_lower', 'ind_ci_upper')], 1, function(row) as.numeric(row)))
df$ind_rank_lower == sapply(1:K, function(x) min(get_ranks(x, tuple_list)$ranks))
df$ind_rank_upper == sapply(1:K, function(x) max(get_ranks(x, tuple_list)$ranks))
```


## Looped coverage

```{r echo = FALSE, eval = FALSE}
get_coverage <- function(dataset_or, K, reps = 5, B = 1000, alpha = 0.1) {
  results_list <- vector("list", reps)  # preallocate list for results
  
  for (iter in 1:reps) {
    set.seed(iter)
    dataset <- dataset_or
    
    # Generate one sample per row
    mat2 <- sapply(1:K, function(i) {
      rnorm(1, mean = dataset[i, 'theta_k'], sd = sqrt(dataset[i, 'variance']))
    })
    
    dataset$otheta_k <- dataset$theta_k
    dataset$theta_k <- mat2  # already a vector, no need to transpose
    
    result <- run_algorithm1(B, dataset, iter)
    
    tuple_list <- t(apply(result[, c('kde_ci_lower', 'kde_ci_upper')], 1, as.numeric))
    
    result$ind_rank_range_length <- sapply(1:K, function(x) {
      length(get_ranks(x, tuple_list)$ranks)
    })
    
    result$otheta_k <- dataset$otheta_k
    
    sorted_ci <- result[order(result$otheta_k), ]
    inside_check <- sum(sorted_ci$otheta_k >= sorted_ci$kde_ci_lower &
                        sorted_ci$otheta_k <= sorted_ci$kde_ci_upper)
    
    t2_val <- get_t2(result$ind_rank_range_length)
    
    results_list[[iter]] <- data.frame(t2 = t2_val, inside = inside_check)
  }
  
  # Combine all iteration results
  do.call(rbind, results_list)
}

looped <- get_coverage(dataset_or, K, reps = 5, B = 1000, alpha = 0.1)
```


```{r echo = FALSE, eval = FALSE}
all_equal(looped, zzz)
```



# References
